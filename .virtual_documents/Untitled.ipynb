import pandas as pd
import glob, os

data_dir = r"C:\Users\mitta\OneDrive\Desktop\MAJOR\physionet.org\files\challenge-2019\1.0.0\training\training_setB"
output_file = "merged_training_setB.csv"

all_files = glob.glob(os.path.join(data_dir, "*.psv"))
print(f"Found {len(all_files)} files")

merged_df = pd.DataFrame()

for idx, file in enumerate(all_files, start=1):
    try:
        df = pd.read_csv(file, sep='|')
        df['patient_id'] = os.path.basename(file).replace('.psv', '')  # Add patient ID
        merged_df = pd.concat([merged_df, df], ignore_index=True)
    except Exception as e:
        print(f"Error reading {file}: {e}")
    
    # Save periodically every 500 files to prevent memory overflow
    if idx % 500 == 0:
        print(f"Processed {idx} files...")
        merged_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)
        merged_df = pd.DataFrame()  # clear from memory

# Save remaining data
if not merged_df.empty:
    merged_df.to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)

print("✅ Merging complete. File saved as:", output_file)



df = pd.read_csv("merged_training_setB.csv", sep=",")
print(df.shape)
df.head()


df.info()
df.isnull().sum().sort_values(ascending=False).head(20)


threshold=0.8*len(df)
df=df.dropna(thresh=threshold,axis=1)


df = df.sort_values(by=["patient_id", "ICULOS"]).reset_index(drop=True)



df = df.groupby("patient_id").apply(lambda group: group.ffill().bfill()).reset_index(drop=True)



df.isnull().sum().sum()


df = df.fillna(df.median(numeric_only=True))



df.isnull().sum().sum()


df.head()


from sklearn.preprocessing import StandardScaler

num_cols = df.select_dtypes(include=['float64','int64']).columns
num_cols = [col for col in num_cols if col not in ['SepsisLabel', 'patient_id']]

scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])



df['Gender'] = df['Gender'].astype('category').cat.codes




print(df.columns.tolist())




df.head().columns



df.to_csv("cleaned_training_setB.csv", index=False)
print("✅ Cleaned dataset saved as cleaned_training_setB.csv")



print(df.shape)
print(df.columns)
df.info()
df.describe()
df['SepsisLabel'].value_counts()



import seaborn as sns
import matplotlib.pyplot as plt

# Drop non-numeric column
df_numeric = df.drop(columns=['patient_id'])

# Plot correlation heatmap
plt.figure(figsize=(10,6))
sns.heatmap(df_numeric.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap of Features")
plt.show()



from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Step 1: Separate features and target
X = df_numeric.drop(columns=['SepsisLabel'])
y = df_numeric['SepsisLabel']

# Step 2: Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 3: Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Training set:", X_train.shape)
print("Test set:", X_test.shape)



import seaborn as sns
import matplotlib.pyplot as plt

cols = ['HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'ICULOS']
for col in cols:
    plt.figure(figsize=(6,4))
    sns.kdeplot(data=df, x=col, hue="SepsisLabel", fill=True)
    plt.title(f"{col} distribution by Sepsis Label")
    plt.show()



sns.countplot(x='SepsisLabel', data=df)
plt.title('Sepsis vs Non-Sepsis Counts')
plt.show()



from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import pandas as pd

# Step 1: Separate features and target
X = df.drop(['SepsisLabel', 'patient_id'], axis=1)
y = df['SepsisLabel']

# Step 2: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 3: Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 4: Apply SMOTE only to training data
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

print("Before SMOTE:", y_train.value_counts())
print("After SMOTE:", y_train_resampled.value_counts())



y_res_df = pd.DataFrame(y_train_resampled, columns=["SepsisLabel"])
sns.countplot(x='SepsisLabel', data=y_res_df)
plt.title('Sepsis vs Non-Sepsis Counts')
plt.show()



from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# ----------------------
# Step 5: Train models
# ----------------------

# Logistic Regression
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_resampled, y_train_resampled)

# Random Forest
rf_model = RandomForestClassifier(random_state=42, n_estimators=100)
rf_model.fit(X_train_resampled, y_train_resampled)

# ----------------------
# Step 6: Predictions
# ----------------------
y_pred_lr = lr_model.predict(X_test_scaled)
y_pred_rf = rf_model.predict(X_test_scaled)

# Optional: get predicted probabilities for ROC AUC
y_prob_lr = lr_model.predict_proba(X_test_scaled)[:,1]
y_prob_rf = rf_model.predict_proba(X_test_scaled)[:,1]

# ----------------------
# Step 7: Evaluation
# ----------------------

print("===== Logistic Regression =====")
print(confusion_matrix(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))
print("ROC AUC:", roc_auc_score(y_test, y_prob_lr))

print("\n===== Random Forest =====")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
print("ROC AUC:", roc_auc_score(y_test, y_prob_rf))



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score

# Models and predictions (assuming these are already trained)
models = {
    "Logistic Regression": (lr_model, y_pred_lr, y_prob_lr),
    "Random Forest": (rf_model, y_pred_rf, y_prob_rf)
}

# Prepare a results dataframe
results_list = []

for name, (model, y_pred, y_prob) in models.items():
    # Classification report as dict
    report = classification_report(y_test, y_pred, output_dict=True)
    
    results_list.append({
        "Model": name,
        "Accuracy": report['accuracy'],
        "Precision (Class 0)": report['0']['precision'],
        "Recall (Class 0)": report['0']['recall'],
        "F1-score (Class 0)": report['0']['f1-score'],
        "Precision (Class 1)": report['1']['precision'],
        "Recall (Class 1)": report['1']['recall'],
        "F1-score (Class 1)": report['1']['f1-score'],
        "ROC AUC": roc_auc_score(y_test, y_prob)
    })

df_results = pd.DataFrame(results_list)
print(df_results)

# Confusion matrix plots
fig, axes = plt.subplots(1, 2, figsize=(14,6))

for ax, (name, (model, y_pred, _)) in zip(axes, models.items()):
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')
    ax.set_title(name)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')

plt.tight_layout()
plt.show()



# Assuming df_results is already created from previous automated code

# Set figure size
plt.figure(figsize=(10,6))

# Bar width
bar_width = 0.35
index = np.arange(2)  # 2 models

# F1-score for Class 1 (minority class)
plt.bar(index, df_results['F1-score (Class 1)'], bar_width, label='F1-score (Class 1)', color='skyblue')

# ROC AUC
plt.bar(index + bar_width, df_results['ROC AUC'], bar_width, label='ROC AUC', color='lightgreen')

# Labels and ticks
plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Comparison of F1-score (Class 1) and ROC AUC')
plt.xticks(index + bar_width / 2, df_results['Model'])
plt.ylim(0,1.1)
plt.legend()
plt.show()



# Predict labels
y_test_pred = rf_model.predict(X_test_scaled)

# Predict probabilities (for ROC AUC or threshold tuning)
y_test_prob = rf_model.predict_proba(X_test_scaled)[:,1]



from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score

# Confusion matrix
cm = confusion_matrix(y_test, y_test_pred)
print("Confusion Matrix:\n", cm)

# Classification report
report = classification_report(y_test, y_test_pred)
print("Classification Report:\n", report)

# ROC AUC score
roc_auc = roc_auc_score(y_test, y_test_prob)
print("ROC AUC:", roc_auc)



import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()



threshold = 0.3  # example, lowers threshold to catch more positives
y_test_pred_new = (y_test_prob >= threshold).astype(int)

print(classification_report(y_test, y_test_pred_new))



import pandas as pd
import numpy as np

# -----------------------------
# 1️⃣ Save training and test data
# -----------------------------
# Combine features and target for saving
train_df = X_train.copy()
train_df['SepsisLabel'] = y_train
train_df.to_excel("training_data.xlsx", index=False)

test_df = X_test.copy()
test_df['SepsisLabel'] = y_test
test_df.to_excel("test_data.xlsx", index=False)

# -----------------------------
# 2️⃣ Make predictions on test set
# -----------------------------
y_test_pred = rf_model.predict(X_test_scaled)
y_test_prob = rf_model.predict_proba(X_test_scaled)[:,1]

# -----------------------------
# 3️⃣ Save test results
# -----------------------------
results_df = X_test.copy()
results_df['Actual'] = y_test
results_df['Predicted'] = y_test_pred
results_df['Predicted_Prob'] = y_test_prob

results_df.to_excel("test_results.xlsx", index=False)

print("All three Excel files created successfully!")




